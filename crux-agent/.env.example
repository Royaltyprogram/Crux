# Application
APP_NAME=crux-agent
ENV=development
DEBUG=True

# LLM Provider Settings
LLM_PROVIDER=openai
MODEL_OPENAI=gpt-4o-mini
MODEL_OPENROUTER=nous-hermes-3b
OPENAI_API_KEY=your-openai-api-key-here
OPENROUTER_API_KEY=your-openrouter-api-key-here

# Redis & Celery
REDIS_URL=redis://localhost:6379/0
CELERY_BROKER_URL=redis://localhost:6379/1
CELERY_RESULT_BACKEND=redis://localhost:6379/2

# Self-Evolve Settings
MAX_ITERS=3
SPECIALIST_MAX_ITERS=2
PROFESSOR_MAX_ITERS=3

# API Settings
API_V1_STR=/api/v1

# CORS Configuration
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Provider-specific settings
OPENAI_MAX_RETRIES=3
OPENAI_TIMEOUT=
OPENROUTER_MAX_RETRIES=3
OPENROUTER_TIMEOUT=900

# For openai api Flex mode 
SERVICE_TIER=free
REASONING_EFFORT=medium

# LMStudio Configuration
# LMSTUDIO_API_KEY=your-lmstudio-api-key-here   # leave blank for local no-auth
MODEL_LMSTUDIO=phi-3-mini-4k-instruct
# LMSTUDIO_MODELS=   # Optional fallback - models are fetched dynamically from server
LMSTUDIO_MAX_RETRIES=3
LMSTUDIO_TIMEOUT=7200
LMSTUDIO_BASE_URL=http://192.168.1.212:1234   # Change to your LMStudio server IP

# Context Limits (tokens) - Configure per provider
LMSTUDIO_CONTEXT_LIMIT=32768     # Default for most LMStudio models
OPENAI_CONTEXT_LIMIT=200000      # Default for OpenAI models (covers most)
OPENROUTER_CONTEXT_LIMIT=262144  # Default for OpenRouter models

# Context Management Settings
SUMMARIZATION_THRESHOLD=0.8      # Trigger summarization at 80% context usage
RESPONSE_RESERVE=1000            # Reserve tokens for response generation

# Production Notes:
# - Set ENV=production for production deployment
# - Set DEBUG=False for production
# - Update CORS_ORIGINS to your actual domains
# - Configure proper Redis URLs for production
